{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b6af60-f174-4dc7-a6b8-2ad51d8af784",
   "metadata": {},
   "source": [
    "# Notebook to run GPT, Gemini, Replicate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b88102b-a07f-4763-9fcb-a2bd248d00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import replicate\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig, HarmBlockThreshold, HarmCategory\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd791ee-3c67-4eb6-8678-827dfbf3bbb7",
   "metadata": {},
   "source": [
    "## Setup all APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d3d6a19-2b3a-4918-8c69-98313903ac9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replicate\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9787e703-fd56-4d6f-82d1-e00ef4137503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT\n",
    "openai_client = OpenAI(api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff0eb4ea-6fea-407a-8de1-7f2ed22b8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini\n",
    "project_id = \"\"   # add project ID and location\n",
    "vertexai.init(project=project_id, location=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0f299-880a-45d6-88a4-7b33448a7fb3",
   "metadata": {},
   "source": [
    "## Prediction Params & Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5141f2a9-a4d2-4d57-b043-9873d213dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for more deterministic output\n",
    "temperature = 0\n",
    "top_p = 1\n",
    "seed = 42\n",
    "max_tokens = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b6e074a-85de-48c7-982e-339dbce560af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = 'You are a cybersecurity expert specializing in cyberthreat intelligence.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcb01b07-3377-4845-a5a9-74719a86ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mapping = {\n",
    "    'llama3-70b': 'meta/meta-llama-3-70b-instruct',\n",
    "    'llama3-8b': 'meta/meta-llama-3-8b-instruct',\n",
    "    'gemini': 'gemini-1.5-pro', \n",
    "    'gpt3': 'gpt-3.5-turbo',\n",
    "    'gpt4': 'gpt-4-turbo'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4890ce6-7c73-4512-9316-64d49cc20607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_prediction(question, model_name):\n",
    "    if model_name.startswith('llama') or model_name.startswith('mistral'):\n",
    "        # replicate\n",
    "        model = model_mapping[model_name]\n",
    "        prompt = sys_prompt + ' ' + question\n",
    "        input = {'prompt': prompt, 'max_tokens': max_tokens, 'temperature': temperature, 'top_p': top_p, 'seed': seed}\n",
    "        output = replicate.run(model, input=input)\n",
    "        output = \"\".join(output)\n",
    "    elif model_name.startswith('gemma'):\n",
    "        # replicate\n",
    "        model = model_mapping[model_name]\n",
    "        prompt = sys_prompt + ' ' + question\n",
    "        input = {'prompt': prompt, 'max_tokens': max_tokens, 'temperature': 0.01, 'top_p': top_p, 'seed': seed}\n",
    "        output = replicate.run(model, input=input)\n",
    "        output = \"\".join(output)\n",
    "    elif model_name.startswith('01-ai'):\n",
    "        # replicate\n",
    "        model = model_mapping[model_name]\n",
    "        prompt = sys_prompt + ' ' + question\n",
    "        input = {'prompt': prompt, 'max_tokens': max_tokens, 'temperature': temperature, 'top_p': top_p, 'seed': seed}\n",
    "        output = replicate.run(model, input=input)\n",
    "        output = \"\".join(output)\n",
    "    elif model_name.startswith('gpt'):\n",
    "        # ChatGPT\n",
    "        model = model_mapping[model_name]\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': sys_prompt},\n",
    "                {'role': 'user', 'content': question}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            seed=seed\n",
    "        )\n",
    "        output = response.choices[0].message.content\n",
    "    elif model_name.startswith('gemini'):\n",
    "        # Gemini\n",
    "        model = model_mapping[model_name]\n",
    "        model = GenerativeModel(model_name=model)\n",
    "        prompt = sys_prompt + ' ' + question\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_output_tokens=max_tokens,\n",
    "        )\n",
    "        safety_settings = {\n",
    "            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "            HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        }\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=generation_config,\n",
    "            safety_settings=safety_settings\n",
    "        )\n",
    "        output = response.text\n",
    "        time.sleep(1)   # so it doesn't throw error\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb481fe-ff69-42af-8306-4eb4f5d48973",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26cd8376-21b9-4c91-b86d-876f1da04260",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = (\n",
    "    \"Analyze the following CVE description and map it to the appropriate CWE. \"\n",
    "    \"Provide a brief justification. The last line of your answer should only contain the CWE ID.\\n\\n\"\n",
    "    \"CVE Description:\\n\\n\"\n",
    "    \"Dell EMC CloudLink 7.1 and all prior versions contain an Improper Input Validation Vulnerability. \"\n",
    "    \"A remote low privileged attacker may potentially exploit this vulnerability, \"\n",
    "    \"leading to execution of arbitrary files on the server.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b473c-9b93-46f9-9ac5-6fb8e3fe314c",
   "metadata": {},
   "source": [
    "##### Are all the APIS working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6699f587-4e5c-4a59-a545-265fa4762fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_single_prediction(question, 'gpt3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79c5ac66-c0a8-4a32-81b0-4c45d68092ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_single_prediction(question, 'gemini'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57c22fb5-bb3d-47f1-bbf2-da27188a0e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_single_prediction(question, 'llama3-8b'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0c673-8d44-43b7-a04a-73f1ec36e2a9",
   "metadata": {},
   "source": [
    "# Run Evaluation for a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86580bd3-20e4-4952-af00-e49810c7703c",
   "metadata": {},
   "source": [
    "### All formatting comes here\n",
    "While these captures most output format of the LLMs we studied, we still had to manually collect some responses from the generated response file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "176c5b73-af68-443f-8b80-f1cfec5df3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rcm(text):\n",
    "    # Define the regex pattern for CWE ID\n",
    "    cwe_pattern = r'CWE-\\d+'\n",
    "\n",
    "    # Find all matches in the text\n",
    "    matches = re.findall(cwe_pattern, text)\n",
    "\n",
    "    # Return the last match if any match is found, otherwise return the original text\n",
    "    if matches:\n",
    "        return matches[-1], True\n",
    "    else:\n",
    "        return text, False\n",
    "\n",
    "def format_vsp(text):\n",
    "    # Define the regex pattern for CVSS v3.1 vector string\n",
    "    #cvss_pattern = r'AV:[^/]+?/AC:[^/]+?/PR:[^/]+?/UI:[^/]+?/S:[^/]+?/C:[^/]+?/I:[^/]+?/A:[^/]+?'\n",
    "    cvss_pattern = r'AV:[A-Za-z]+/AC:[A-Za-z]+/PR:[A-Za-z]+/UI:[A-Za-z]+/S:[A-Za-z]+/C:[A-Za-z]+/I:[A-Za-z]+/A:[A-Za-z]+'\n",
    "\n",
    "\n",
    "    # Find all matches in the text\n",
    "    matches = re.findall(cvss_pattern, text)\n",
    "\n",
    "    # Return the last match if any match is found, otherwise return the original text\n",
    "    if matches:\n",
    "        return matches[-1], True\n",
    "    else:\n",
    "        return text, False\n",
    "\n",
    "def format_mcq(text):\n",
    "    last_line = text.split('\\n')[-1].rstrip()\n",
    "    if last_line.startswith('A)') or last_line.startswith('B)') or last_line.startswith('C)') or last_line.startswith('D)'):\n",
    "        return last_line[0]\n",
    "    if last_line.endswith('A') or last_line.endswith('B') or last_line.endswith('C') or last_line.endswith('D'):\n",
    "        return last_line[-1]\n",
    "    if last_line.endswith('**'):\n",
    "        return last_line[-3]\n",
    "    if len(last_line) == 0:\n",
    "        last_line = text.split('\\n')[-2].rstrip()\n",
    "        if last_line.startswith('A)') or last_line.startswith('B)') or last_line.startswith('C)') or last_line.startswith('D)'):\n",
    "            return last_line[0]\n",
    "        if last_line.endswith('A') or last_line.endswith('B') or last_line.endswith('C') or last_line.endswith('D'):\n",
    "            return last_line[-1]\n",
    "        if last_line.endswith('**'):\n",
    "            return last_line[-3]\n",
    "    return ' '.join(text.split('\\n'))\n",
    "\n",
    "def format_taa(text):\n",
    "    # need to manually extract the attribution\n",
    "    return ' '.join(text.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc6068fb-49a8-4d80-b7d4-0d5f58750a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(file_path, task, model_name):\n",
    "    # Keep track of time and total #chars generated\n",
    "    start_time = time.time()\n",
    "    count_chars = 0\n",
    "    instructions_failed = 0\n",
    "    \n",
    "    data = pd.read_csv(file_path, encoding='utf-8', sep='\\t')\n",
    "\n",
    "    # response contain the entire response, result the formatted result\n",
    "    all_responses = []\n",
    "    all_results = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        prompt = row['Prompt']\n",
    "        try:\n",
    "            output = get_single_prediction(prompt, model_name)\n",
    "            \n",
    "            count_chars += len(output)\n",
    "            all_responses.append(output)\n",
    "            if task == 'rcm':\n",
    "                answer, success = format_rcm(output)\n",
    "                if not success:\n",
    "                    instructions_failed += 1\n",
    "            elif task == 'vsp':\n",
    "                answer, success = format_vsp(output)\n",
    "                if not success:\n",
    "                    instructions_failed += 1      \n",
    "            elif task == 'mcq':\n",
    "                answer = format_mcq(output)\n",
    "            elif task == 'taa':\n",
    "                answer = format_taa(output)\n",
    "            else:\n",
    "                raise ValueError('Task unknown!')\n",
    "        except Exception as e:\n",
    "            answer = 'Error'\n",
    "            all_responses.append(answer)\n",
    "            print('Exception at row ', index+1)\n",
    "            print(e)\n",
    "        all_results.append(answer)\n",
    "        print(index+1, answer)\n",
    "        # print(index+1)\n",
    "\n",
    "\n",
    "    time_taken = time.time() - start_time\n",
    "    print('Time taken:', time_taken)\n",
    "    print('#Characters generated:', count_chars)\n",
    "    print('#Instructions failed:', instructions_failed)\n",
    "\n",
    "    # Save all the responses & results\n",
    "    out_response = file_path.split('.')[0] + '_' + model_name + '_response.txt'\n",
    "    out_result = file_path.split('.')[0] + '_' + model_name + '_result.txt'\n",
    "\n",
    "    with open(out_response, 'w', encoding='utf-8') as f:\n",
    "        out_str = ''\n",
    "        for i in range(len(all_responses)):\n",
    "            out_str += '#####' + str(i+1) + '#####\\n'\n",
    "            out_str += all_responses[i]\n",
    "            out_str += '\\n\\n'\n",
    "        f.write(out_str)\n",
    "    with open(out_result, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(all_results))\n",
    "\n",
    "    print('------- Done --------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef874d9-4a0a-4889-971b-3781410b0a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6665c28d-4d41-4b98-b150-94d12e9c3ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_evaluation('datasets/cti-mcq.tsv', 'mcq', 'gpt3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d94b102e-4fef-4d9e-8dc3-250b9b7edcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_evaluation('datasets/cti-rcm.tsv', 'rcm', 'gpt3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3123f72-0838-4fc1-8ebe-912219909344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_evaluation('datasets/cti-vsp.tsv', 'vsp', 'gpt3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "752819a6-5082-4e80-9501-c65720079fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_evaluation('datasets/cti-taa.tsv', 'taa', 'gpt3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9be07b-6030-46c5-8442-d743875e8ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
